This folder is to run local LLM

# Todo
- using streamlit to create a converstional robot on the local machine.

# Building own Gemma env running locally

## setup environment 
- colone or create a 'mlx' conda env.
- if the virtual env doesn't include pytorch, please install [PyTorch](https://pytorch.org) so as to accelerate the speed. 

## helpful tool
- use the following to monitor memory consumption.
$top -o mem 

# Resource
- [MLX Community on Huggingface](https://huggingface.co/mlx-community)
- huggingface add Gemma into the supported model
- 